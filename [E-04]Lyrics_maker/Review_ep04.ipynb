{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "710cc3a5",
   "metadata": {},
   "source": [
    "### 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e686ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Citizen:', 'Before we proceed any further, hear me speak.', '', 'All:', 'Speak, speak.', '', 'First Citizen:', 'You are all resolved rather to die than to famish?', '']\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 파일을 읽기모드로 열고\n",
    "# 라인 단위로 끊어 list 형태로 읽어오기\n",
    "file_path = 'shakespeare.txt'\n",
    "with open(file_path, 'r') as f:\n",
    "    raw_corpus = f.read().splitlines()\n",
    "    \n",
    "print(raw_corpus[:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f0c2ce",
   "metadata": {},
   "source": [
    "> - 원치 않는 문장\n",
    "    - 화자가 표기된 문장: `'First Citizen:'`, `'All:'`\n",
    "        - 문장의 끝이 `:`로 끝남 -> 이를 기준으로 문장 제외\n",
    "    - 공백인 문장: `''`\n",
    "        - 길이가 0이라면 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2c87ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before we proceed any further, hear me speak.\n",
      "Speak, speak.\n",
      "You are all resolved rather to die than to famish?\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue # 공백이면 건너뛰기\n",
    "    if sentence[-1] == ':': continue # 화자라면 건너뛰기\n",
    "    \n",
    "    if idx > 9: break # 일단 문장 10개만 확인해보기\n",
    "    \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde3542d",
   "metadata": {},
   "source": [
    "> **토큰화(Tokenize)** : 문장을 일정한 기준으로 쪼개기\n",
    "> - 가장 간단하게 띄어쓰기 기준으로 쪼개기\n",
    "\n",
    "> - `Hi,` 와 같이 문장 부호 -> 문장 부호 양쪽에 공백 추가\n",
    "> - `First`와 `first`를 다른 단어로 인식 -> 모든 문자 소문자로 변환\n",
    "> - `ten-year-old` 를 한 단어로 인식 -> 특수 문자 모두 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81de9731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence .  <end>\n"
     ]
    }
   ],
   "source": [
    "# 1. 소문자로 변환. 양쪽 공백 지우기\n",
    "# 2. 특수 문자([?.!,¿]) 양쪽에 공백 넣기\n",
    "    # r\" \\1 \": \\1이란 정규식의 첫 번째 그룹 -> 양쪽에 공백 넣기\n",
    "# 3. 여러 개 공백 -> 한 개로\n",
    "# 4. a-zA-Z?.!,¿ 가 아닌 모든 문자는 하나의 공백으로 (특수문자 제거)\n",
    "# 5. 문장 시작에는 <start>, 끝에는 <end>\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a400f696",
   "metadata": {},
   "source": [
    "> - 소스 문장(Source Sentence): 모델의 입력이 되는 문장\n",
    "    - `<start>`를 없애면\n",
    "> - 타겟 문장(Traget Sentence): 모델의 출력 문장\n",
    "    - `<end>`를 없애면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe880437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> before we proceed any further , hear me speak .  <end>',\n",
       " '<start> speak , speak .  <end>',\n",
       " '<start> you are all resolved rather to die than to famish ?  <end>',\n",
       " '<start> resolved . resolved .  <end>',\n",
       " '<start> first , you know caius marcius is chief enemy to the people .  <end>',\n",
       " '<start> we know t , we know t .  <end>',\n",
       " '<start> let us kill him , and we ll have corn at our own price .  <end>',\n",
       " '<start> is t a verdict ?  <end>',\n",
       " '<start> no more talking on t let it be done away , away !  <end>',\n",
       " '<start> one word , good citizens .  <end>']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정제된 문장 모으기\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == ':': continue\n",
    "    \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad08184",
   "metadata": {},
   "source": [
    "> `tf.keras.preprocessing.text.Tokenizer` 패키지\n",
    "> - 정제된 데이터를 토큰화, 단어사전 만들고, 데이터 숫자 변환까지 한 번에\n",
    "> - **벡터화(vectorize)** , **텐서(tensor)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3590221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40 ...    0    0    0]\n",
      " [   2  110    4 ...    0    0    0]\n",
      " [   2   11   50 ...    0    0    0]\n",
      " ...\n",
      " [   2  149 4553 ...    0    0    0]\n",
      " [   2   34   71 ...    0    0    0]\n",
      " [   2  945   34 ...    0    0    0]] <keras.preprocessing.text.Tokenizer object at 0x7f8d88901a50>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 7000단어를 기억할 수 있는 tokenizer 만들기\n",
    "    # 문장 이미 정제했으니 filters 필요 x\n",
    "    # 7000단어에 포함되지 못한 단어는 '<unk>'로 바꾸기\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=7000,\n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\")\n",
    "\n",
    "    # corpus를 이용해 tokenizer 내부 단어장 완성하기\n",
    "    tokenizer.fit_on_texts(corpus) # 문자를 받아 리스트 형태로 반환\n",
    "    \n",
    "    # 준비한 tokenizer로 corpus를 Tensor로 반환하기\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    print(tensor, tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc4796f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40  933  140  591    4  124   24  110]\n",
      " [   2  110    4  110    5    3    0    0    0    0]\n",
      " [   2   11   50   43 1201  316    9  201   74    9]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f24011d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : .\n",
      "6 : the\n",
      "7 : and\n",
      "8 : i\n",
      "9 : to\n",
      "10 : of\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, ':', tokenizer.index_word[idx])\n",
    "    d\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2db3f5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0   0\n",
      "   0   0]\n"
     ]
    }
   ],
   "source": [
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높음\n",
    "src_input = tensor[:, :-1]\n",
    "\n",
    "# <start> 잘라내기\n",
    "tgt_input = tensor[:, 1:]\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096fa557",
   "metadata": {},
   "source": [
    "> 데이터셋 객체 생성\n",
    "> - `model.fit()`로 numpy array 데이터셋을 생성하지 않고,\n",
    "> - `tf.data.Dataset`로 텐서 데이터 객체 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df1ad7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-06 16:14:41.423323: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(256, 20), dtype=tf.int32, name=None), TensorSpec(shape=(256, 20), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "# 0(<pad>) 포함\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "# 데이터셋 만들기\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eb6b24",
   "metadata": {},
   "source": [
    "> **정리**\n",
    "> 1. 정규표현식을 이용한 corpus 생성\n",
    "> 2. `tf.keras.preprocessing.text.Tokenizer`를 이용해 corpus를 텐서로 변환\n",
    "> 3. `tf.data.Dataset.from_tensor_slices()`를 이용해 corpus 텐서를 `tf.data.Dataset` 객체로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb07d68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d93d77c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44a28de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e046d543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2148d617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3662cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985d9a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3518aaa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
